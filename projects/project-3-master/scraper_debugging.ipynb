{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# API\n",
    "import requests\n",
    "\n",
    "# Automating\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "class reddit_scraper:\n",
    "    \n",
    "    def __init__(self,subreddit, n_iter, epoch_right_now):\n",
    "        '''\n",
    "        I recommend n_iter to be low, no higher than 10 \n",
    "        \n",
    "        n_iter  = how many times you want to scrape 100 reddit messages\n",
    "        epoch_right_now = start here\n",
    "        subreddit = name of subreddit in a string\n",
    "        '''\n",
    "        self.subreddit = subreddit\n",
    "        self.n_iter = n_iter\n",
    "        self.epoch_start = epoch_right_now\n",
    "        self.epoch_earliest = 0\n",
    "        \n",
    "        # check for folder, is it does not exist make the folder\n",
    "        try:\n",
    "            os.makedirs(f'./data/reddit/{self.subreddit}', exist_ok=True)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def get_comments(self, subreddit, n_iter, epoch_right_now): # subreddit name and number of times function should run\n",
    "\n",
    "        # store base url variable\n",
    "        base_url = 'https://api.pushshift.io/reddit/search/submission/?subreddit='\n",
    "        # instantiate empty list    \n",
    "        df_list = []\n",
    "        # save current epoch, \n",
    "        current_time = epoch_right_now\n",
    "\n",
    "        # set up for loop\n",
    "        for post in range(n_iter):\n",
    "            # instantiate get request\n",
    "            #time.sleep(5)\n",
    "            res = requests.get(\n",
    "                # requests.get takes base_url and params\n",
    "                base_url,\n",
    "                # parameters for get request\n",
    "                params={ \n",
    "                    # specify subreddit\n",
    "                    'subreddit': subreddit,\n",
    "                    # specify number of posts to pull\n",
    "                    'size': 100,\n",
    "                    # ???\n",
    "                    'lang': True,\n",
    "                    # pull everything from current time backward\n",
    "                    'before': current_time})\n",
    "\n",
    "            # take data from most recent request, store as df\n",
    "            df = pd.DataFrame(res.json()['data'])\n",
    "\n",
    "            # pull specific columns from dataframe for analysis\n",
    "            try:\n",
    "                df_filtered = df.T.loc[['title', 'created_utc', 'selftext', 'subreddit', 'media_only', 'author', 'permalink']]\n",
    "            except:\n",
    "                df_filtered = df.T.loc[['title', 'created_utc', 'selftext', 'subreddit', 'author', 'permalink']]\n",
    "                #print(f'Stopped at {self.epoch_earliest}')\n",
    "                \n",
    "            # append to empty dataframe list\n",
    "            df_list.append(df_filtered)\n",
    "\n",
    "            # set current time counter back to last epoch in recently grabbed df\n",
    "            current_time = df['created_utc'].min()\n",
    "            self.epoch_earliest = current_time\n",
    "        # return one dataframe for all requests\n",
    "        df = pd.concat(df_list, axis=1)\n",
    "        df = df.T\n",
    "        return df.reset_index()\n",
    "    # Adapated from Tim Book's Lesson Example\n",
    "\n",
    "    def reddit_getter(self,sessions):\n",
    "        '''\n",
    "        This function multiplies n_iter by sessions, resulting in a total amount of requested scraped datapoints\n",
    "        \n",
    "        The amount scraped each request is 100 comments and their associated metadata\n",
    "        \n",
    "        The sessions define how many scraping sessions there will be\n",
    "        \n",
    "        Eg: n_iter = 10, sessions = 1 # hard define request = 100\n",
    "        Total Scraped Message = 10 * 100 * 1 = 1000\n",
    "        \n",
    "        n_iter = 10, sessions = 100\n",
    "        Total Scraped Messages = 10 * 100 * 1000 = 1000000\n",
    "        \n",
    "        Use this function to contuinopusly scrape because it has a wait timer to ensure reddit is not overloaded. It is set to a 2 second wait \n",
    "        '''\n",
    "                \n",
    "        for i in range(sessions):\n",
    "            print('earliest',rs.epoch_earliest,'starting',rs.epoch_start)\n",
    "            # scrape reddit based on inputted parameters when instantiating class\n",
    "            time.sleep(5)\n",
    "            scraped = rs.get_comments(subreddit='askengineers', n_iter=rs.n_iter, epoch_right_now=rs.epoch_start)\n",
    "\n",
    "            #after messages have been scraped, store amount into a single csv, \n",
    "            scraped.to_csv(f'./data/reddit/{rs.subreddit}/comments_epoch_{rs.epoch_earliest}-{rs.epoch_start}.csv', index=False)\n",
    "\n",
    "            # define the starting point as the epoch with the smallest epoch and go backwards again\n",
    "            #print('earliest',rs.epoch_earliest, 'starting',rs.epoch_start)\n",
    "            rs.epoch_start = rs.epoch_earliest\n",
    "            \n",
    "            # print out log for user to monitor amount collectec\n",
    "            print(f'Scraped {(i+1) * self.n_iter * 100} of {sessions*self.n_iter*100} requested comments from subreddit {self.subreddit}')\n",
    "        return f'Scraping complete, collected {self.n_iter * sessions*100} of reddit comments from subreddit {self.subreddit}'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = reddit_scraper(n_iter=10, epoch_right_now=1503598550 , subreddit='askengineers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = rs.get_comments(subreddit='askengineers', n_iter=rs.n_iter, epoch_right_now=rs.epoch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv(f'./data/reddit/comments_{rs.subreddit}_{datetime.datetime.now()}_epoch_{rs.epoch_earliest}-{rs.epoch_start}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earliest 0 starting 1526260045\n",
      "Scraped 1000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1523485563 starting 1523485563\n",
      "Scraped 2000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1520915593 starting 1520915593\n",
      "Scraped 3000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1518096253 starting 1518096253\n",
      "Scraped 4000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1514889337 starting 1514889337\n",
      "Scraped 5000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1510759793 starting 1510759793\n",
      "Scraped 6000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1507430977 starting 1507430977\n",
      "Scraped 7000 of 8000 requested comments from subreddit askengineers\n",
      "earliest 1503598550 starting 1503598550\n",
      "Scraped 8000 of 8000 requested comments from subreddit askengineers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraping complete, collected 8000 of reddit comments from subreddit askengineers'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.reddit_getter(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwu = pd.read_csv('./data/reddit/askengineers/comments_2020-10-01 18:19:16.028963_epoch_1600265449-1601587311.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.epoch_start - rs.epoch_earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.epoch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.epoch_earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
